{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8kbrJBIhudc"
      },
      "source": [
        " HORNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjJUovV2g8cD"
      },
      "source": [
        "#1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HNIRS_7hAAs"
      },
      "source": [
        "#from high_order_rnn import *\n",
        "#from __init__ import *\n",
        "#2\n",
        "from high_order_rnn_cell import *\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "69RCV7r-uNwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwBAcVruJjLm"
      },
      "source": [
        "#3\n",
        "# prepare train data to pass to network\n",
        "# in this part we prepare train data in form:(number of data,time steps,number of data in each time step)\n",
        "from numpy import loadtxt\n",
        "import random\n",
        "import numpy\n",
        "import os\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "tf.random.set_seed(1234)\n",
        "min1_in=[]\n",
        "max1_in=[]\n",
        "min1_out=[]\n",
        "max1_out=[]\n",
        "X=[]\n",
        "Y=[]\n",
        "x_plot=[]\n",
        "temp_X=[]\n",
        "temp_Y=[]\n",
        "st='/content/drive/My Drive/18_5/Train/'#path to folder of train data\n",
        "\n",
        "for filename in os.listdir(st):\n",
        "    # mydata contains 3 lists which ,each one is a column of data files\n",
        "    # (in data files we have:column 1 as time,column 2 as input ,column3 as output)\n",
        "    # because value of times is not important we ignore them(mydata[0 is ignored])\n",
        "    mydata = loadtxt(st+filename, unpack=True)\n",
        "    my_input=mydata[1]\n",
        "    inputx=[]\n",
        "    # set input in desired form for network:\n",
        "    # input shape should be:(number of data,time steps,number of data in each time step)\n",
        "    # for example we have 12 data,each has 201 time step ,in each time step we have one input\n",
        "    # so input shape is:(12,200,1)..\n",
        "    x_plot.append(mydata[0])\n",
        "\n",
        "    for i in my_input:\n",
        "        inputx.append([i]) # NOTICE:If your data has more than one input,\n",
        "        #add all of them like:inputx.append([i,j,k])which i,j ,k,.. are inputs of your data\n",
        "\n",
        "    my_output=mydata[2]#select output(2d list in mydata which is 3d column in data file)\n",
        "    # if you normalized inputs,so normalize output like below:\n",
        "    # my_output=(my_output - numpy.min(my_output)) / (numpy.max(my_output) - numpy.min(my_output))\n",
        "    outputy=[]\n",
        "    for i in my_output:\n",
        "        outputy.append([i])\n",
        "    fig = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig.add_subplot(211)\n",
        "    ax0.set_title('Train Data (input1) without normalization')\n",
        "    ax0.plot(mydata[0],my_input,color='b',lw=1)\n",
        "    ax0.grid()\n",
        "    #plt.legend([\"predicted\",\"target value\"])\n",
        "    plt.show()\n",
        "    fig2 = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig2.add_subplot(211)\n",
        "    ax0.set_title('Train Data (output) without normalization')\n",
        "    ax0.plot(mydata[0],my_output,color='b',lw=1)\n",
        "\n",
        "    ax0.grid()\n",
        "    #plt.legend([\"predicted\",\"target value\"])\n",
        "    plt.show()\n",
        "    min1_out.append(numpy.min(my_output))\n",
        "    max1_out.append(numpy.max(my_output))\n",
        "    min1_in.append(numpy.min(my_input))\n",
        "    max1_in.append(numpy.max(my_input))\n",
        "    temp_X.append(my_input)\n",
        "    temp_Y.append(my_output)\n",
        "    X.append(inputx)# train input\n",
        "    Y.append(outputy)#train  output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wnAYAmjKj4w"
      },
      "source": [
        "#4\n",
        "# implement normalization in range[0,1] on train data\n",
        "mymin1_in=numpy.min(min1_in)\n",
        "mymax1_in=numpy.max(max1_in)\n",
        "mymin1_out=numpy.min(min1_out)\n",
        "mymax1_out=numpy.max(max1_out)\n",
        "input_seq2=temp_X\n",
        "target_seq2=temp_Y\n",
        "X=[]\n",
        "Y=[]\n",
        "for i in range(len(input_seq2)):\n",
        "    input_seq2[i]=(input_seq2[i] - mymin1_in) / (mymax1_in - mymin1_in)\n",
        "    target_seq2[i]=(target_seq2[i] - mymin1_out) / (mymax1_out - mymin1_out)\n",
        "    outputy=[]\n",
        "    for j in target_seq2[i]:\n",
        "        outputy.append([j])\n",
        "    inputx=[]\n",
        "    for j in input_seq2[i]:\n",
        "        inputx.append([j])\n",
        "    X.append(inputx)# train input\n",
        "    Y.append(outputy)#train  output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quE5pn5IJu-A"
      },
      "source": [
        "#5\n",
        "# prepare test data to pass to network\n",
        "# in this part we prepare train data in form:(number of data,time steps,number of data in each time step)\n",
        "from numpy import loadtxt\n",
        "import random\n",
        "import numpy\n",
        "import os\n",
        "from scipy import signal\n",
        "\n",
        "X_test=[]\n",
        "Y_test=[]\n",
        "x_plot_test=[]\n",
        "temp_X2=[]\n",
        "temp_Y2=[]\n",
        "\n",
        "st='/content/drive/My Drive/18_5/Test/'#path to folder of train data\n",
        "\n",
        "for filename in os.listdir(st):\n",
        "    # mydata contains 3 lists which ,each one is a column of data files\n",
        "    # (in data files we have:column 1 as time,column 2 as input ,column3 as output)\n",
        "    # because value of times is not important we ignore them(mydata[0 is ignored])\n",
        "    mydata = loadtxt(st+filename, unpack=True)\n",
        "    x_plot_test.append(mydata[0])\n",
        "    my_input=mydata[1]\n",
        "    inputx=[]\n",
        "    # set input in desired form for network:\n",
        "    # input shape should be:(number of data,time steps,number of data in each time step)\n",
        "    # for example we have 12 data,each has 201 time step ,in each time step we have one input\n",
        "    # so input shape is:(12,200,1)..\n",
        "    x_plot.append(mydata[0])\n",
        "\n",
        "    for i in my_input:\n",
        "        inputx.append([i]) # NOTICE:If your data has more than one input,add all of them like:inputx.append([i,j,k])which i,j ,k,.. are inputs of your data\n",
        "\n",
        "    my_output=mydata[2]#select output(2d list in mydata which is 3d column in data file)\n",
        "    # if you normalized inputs,so normalize output like below:\n",
        "    # my_output=(my_output - numpy.min(my_output)) / (numpy.max(my_output) - numpy.min(my_output))\n",
        "    outputy=[]\n",
        "    for i in my_output:\n",
        "        outputy.append([i])\n",
        "    fig = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig.add_subplot(211)\n",
        "    ax0.set_title('Test Data (input1) without normalization')\n",
        "    ax0.plot(mydata[0],my_input,color='b',lw=1)\n",
        "    ax0.grid()\n",
        "    #plt.legend([\"predicted\",\"target value\"])\n",
        "    plt.show()\n",
        "    fig2 = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig2.add_subplot(211)\n",
        "    ax0.set_title('Test Data (output) without normalization')\n",
        "    ax0.plot(mydata[0],my_output,color='b',lw=1)\n",
        "\n",
        "    ax0.grid()\n",
        "    #plt.legend([\"predicted\",\"target value\"])\n",
        "    plt.show()\n",
        "    temp_X2.append(inputx)\n",
        "    temp_Y2.append(outputy)\n",
        "    X_test.append(inputx)# test input\n",
        "    Y_test.append(outputy)#test  output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6p18ubMMan"
      },
      "source": [
        "#6\n",
        "# implement normalization in range[0,1] on test data\n",
        "input_seq22=temp_X2\n",
        "target_seq22=temp_Y2\n",
        "X_test=[]\n",
        "Y_test=[]\n",
        "for i in range(len(input_seq22)):\n",
        "    input_seq22[i]=(input_seq22[i] - mymin1_in) / (mymax1_in - mymin1_in)\n",
        "    target_seq22[i]=(target_seq22[i] - mymin1_out) / (mymax1_out - mymin1_out)\n",
        "    outputy=[]\n",
        "    for j in target_seq22[i]:\n",
        "        outputy.append([j])\n",
        "    inputx=[]\n",
        "    for j in input_seq22[i]:\n",
        "        inputx.append([j])\n",
        "    X_test.append(inputx)# test input\n",
        "    Y_test.append(outputy)#test  output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkKIUyv1NPv0"
      },
      "source": [
        "    #7\n",
        "    # here we interpolate data to all data have same lenght\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65 # number of interpolation (changes data to 200 time step)\n",
        "    X_new=[]\n",
        "    for i in X:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "        xx=[]\n",
        "        for j in new_y:\n",
        "          xx.append([j])\n",
        "        X_new.append(xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u3UqeqfNSx6"
      },
      "source": [
        "    #8\n",
        "    # iterpolate test input data\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65\n",
        "    X_new_test=[]\n",
        "    for i in X_test:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "        xx=[]\n",
        "        for j in new_y:\n",
        "          xx.append([j])\n",
        "        X_new_test.append(xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkIYAuX2NY7E"
      },
      "source": [
        "    #9\n",
        "    # # iterpolate train output data\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65\n",
        "    Y_new=[]\n",
        "    for i in Y:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "        xx=[]\n",
        "        for j in new_y:\n",
        "          xx.append([j])\n",
        "        Y_new.append(xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vpCZYC1NcJD"
      },
      "source": [
        "    #10\n",
        "    # # iterpolate test output data\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65\n",
        "    Y_new_test=[]\n",
        "    for i in Y_test:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "        xx=[]\n",
        "        for j in new_y:\n",
        "          xx.append([j])\n",
        "        Y_new_test.append(xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZBLPMy-Ng_v"
      },
      "source": [
        "    #11\n",
        "    # iterpolate train time\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65\n",
        "    X_plot_new=[]\n",
        "    for i in x_plot:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "\n",
        "        X_plot_new.append(new_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOim619LNjtb"
      },
      "source": [
        "    #12\n",
        "    # iterpolate test time\n",
        "    from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "    import numpy as np\n",
        "    inter_len=65\n",
        "    X_plot_new_test=[]\n",
        "    for i in x_plot_test:\n",
        "        x=range(0,len(i))# number of time steps for original time serie\n",
        "        y=i\n",
        "        interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "        new_x = np.arange(0,len(x),len(x)/inter_len)\n",
        "        new_y = interpolation_function(new_x)\n",
        "        if(len(new_y)>inter_len):\n",
        "            interpolation_function = InterpolatedUnivariateSpline(x,y,k=2)\n",
        "            new_x = np.arange(0,len(x),(len(x)/inter_len)+0.003)\n",
        "            new_y = interpolation_function(new_x)\n",
        "\n",
        "        X_plot_new_test.append(new_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi8aRLgsezGa"
      },
      "source": [
        "#13\n",
        "X=X_new\n",
        "Y=Y_new\n",
        "X_test=X_new_test\n",
        "Y_test=Y_new_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Y1bkAyW88o"
      },
      "source": [
        "#14\n",
        "import numpy\n",
        "print(\"train input shape:\",numpy.array(X).shape)\n",
        "print(\"train output shape\",numpy.array(Y).shape)\n",
        "print(\"test input shape:\",numpy.array(X_test).shape)\n",
        "print(\"test output shape\",numpy.array(Y_test).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YGkB5d4eZQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8EkoguyvB2O"
      },
      "source": [
        "#15\n",
        "import tensorflow\n",
        "#import keras\n",
        "#from keras.models import Sequential\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adamax, RMSprop\n",
        "from tensorflow.keras import layers,Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "#from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, RNN\n",
        "from tensorflow.python.keras.layers import recurrent_v2\n",
        "from tensorflow.python.ops import rnn_cell_impl\n",
        "import time\n",
        "import errno\n",
        "\n",
        "# ALL THE STEPS ARE SAME AS MLP,THE ONLY DIFFERENCE IS IN USING RNN.\n",
        "# create network:\n",
        "# input_shape=[number_timestep,number_input]\n",
        "# for example input_shape=[201,1] :means our data have 201 time step and each time step has one value\n",
        "\n",
        "# return_sequences=True: because we are modeling a time serie and need output of all steps\n",
        "# so each step of network must returen output in that step(contrary to classification)\n",
        "# if we have classification problem we set return_sequences=False,to have only output of last step(which is class)\n",
        "\n",
        "#.layers.Dense(1) output layer of each time step has one output\n",
        "# notice:because we we want to predict a float value,so we dont use activation function in last layer\n",
        "number_timestep=len(X[0])# WHEN GENERATING DATA,TRY TO HAVE REASONABLE TIMESTEPS(LIKE 500 NOT 25000!)\n",
        "number_input=len(X[0][0])\n",
        "\n",
        "model = tensorflow.keras.Sequential()\n",
        "\n",
        "model.add(RNN(HORNNCell(25, order=3, activation='sigmoid'), input_shape=[number_timestep,number_input], return_sequences=True))\n",
        "model.add(RNN(HORNNCell(25, order=3, activation='sigmoid'), input_shape=[number_timestep,number_input], return_sequences=True))\n",
        "model.add(RNN(HORNNCell(5, order=1, activation='sigmoid'), input_shape=[number_timestep,number_input], return_sequences=True))\n",
        "model.add(tensorflow.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.__version__"
      ],
      "metadata": {
        "id": "RKB0fRpwAg_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc42J0YjvKMW"
      },
      "source": [
        "model.compile(loss='huber',\n",
        "              optimizer=RMSprop(learning_rate=0.005),\n",
        "              metrics=['mean_absolute_percentage_error'],)\n",
        "\n",
        "batch_size=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFHWUBm3xHRw"
      },
      "source": [
        "#validation_data : a part of data which is used to monitor model during training(arbitrary)\n",
        "#ModelCheckpoint: save model to be used later(save_best_only=True saves network based on best result )\n",
        "#  monitor='val_loss':saves model based on loss of validation data\n",
        "\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/My Drive/HORNN.hdf5',\n",
        "                                   monitor='val_loss', save_best_only=True, restore_best_weights = True)\n",
        "\n",
        "\n",
        "start_time=time.time()\n",
        "\n",
        "history = model.fit(X,Y,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,validation_data=(X_test,Y_test),\n",
        "                    callbacks=[ModelCheckpoint('/content/drive/My Drive/HORNN.hdf5', monitor='val_loss', save_best_only=True, save_weights_only=True)])\n",
        "\n",
        "end_time = time.time()\n",
        "total_time= end_time-start_time\n",
        "print(total_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV7GhDhfwn3s"
      },
      "source": [
        "\n",
        "from timeit import default_timer as timer\n",
        "start1 = timer()\n",
        "tainy_predict1 = model(np.array(X[0]), training = False)\n",
        "end1 = timer()\n",
        "print('Direct model runtime :', end1 - start1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BkRy9XuxRPU"
      },
      "source": [
        "#17\n",
        "start2 = timer()\n",
        "tainy_predict2 = model.predict(X[1], verbose=0)\n",
        "end2 = timer()\n",
        "print('Model.predict runtime :', end2 - start2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvEHjpRDtqz8"
      },
      "source": [
        "#18\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cnFghRdtDyR"
      },
      "source": [
        "#19\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch,history.history['loss'],label='loss')\n",
        "plt.plot(history.epoch,history.history['val_loss'],label='val_loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch,history.history['mean_absolute_percentage_error'],label='mean_absolute_percentage_error')\n",
        "plt.plot(history.epoch,history.history['val_mean_absolute_percentage_error'],label='val_mean_absolute_percentage_error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-y8cfXTuH47"
      },
      "source": [
        "#20\n",
        "filepath = '/content/drive/My Drive/HORNN.hdf5'\n",
        "custom_objects = {'HORNNCell':HORNNCell}\n",
        "with keras.utils.custom_object_scope(custom_objects):\n",
        "  mymodel=tf.keras.models.clone_model(model)\n",
        "  mymodel.load_weights('/content/drive/My Drive/HORNN.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22\n",
        "# plot the result on train data\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse_train=0\n",
        "#predicted=mymodel.predict(X)\n",
        "predicted=mymodel.predict(X)\n",
        "#mm=[]\n",
        "for i in range(len(X)):\n",
        "    fig = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig.add_subplot(211)\n",
        "#    aa=numpy.array(predicted[i])\n",
        "    bb=numpy.array(Y[i])\n",
        "    cc=numpy.array(predicted[i])\n",
        "    ax0.set_title('Train Data')\n",
        "#    ax0.plot(X_plot_new[i],aa,color='y',lw=3,label='NN Output',linestyle=':')\n",
        "    ax0.plot(X_plot_new[i],bb,color='b',marker='None',lw=2,markersize=2,label='Train Data')\n",
        "    ax0.plot(X_plot_new[i],cc,color='r',lw=3,label='NN Output',linestyle=':')\n",
        "    ax0.grid()\n",
        "    plt.legend([\"target\",\"HORNN\"])\n",
        "    plt.show()\n",
        "    mse_train+=mean_squared_error(bb,cc)\n",
        "#    mm.append([X_plot_new[i],aa,cc])\n",
        "print('total mse train:',mse_train/len(X))"
      ],
      "metadata": {
        "id": "TReWUKv25zIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22\n",
        "# plot the result on test data\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse_test=0\n",
        "#predicted=model.predict(X_test)\n",
        "predicted=mymodel.predict(X_test)\n",
        "#mm=[]\n",
        "for i in range(len(X_test)):\n",
        "    fig = plt.figure(figsize=(11,7))\n",
        "    ax0 = fig.add_subplot(211)\n",
        "\n",
        "#    aa=numpy.array(predicted[i])\n",
        "    bb=numpy.array(Y_test[i])\n",
        "    cc=numpy.array(predicted[i])\n",
        "    ax0.set_title('Test Data')\n",
        "#    ax0.plot(X_plot_new_test[i],aa,color='g',lw=3,label='NN Output',linestyle='-')\n",
        "    ax0.plot(X_plot_new_test[i],bb,color='b',marker='None',lw=2,markersize=2,label='Test Data')\n",
        "    ax0.plot(X_plot_new_test[i],cc,color='r',lw=3,label='NN Output',linestyle=':')\n",
        "    ax0.grid()\n",
        "    plt.legend([\"target value\",\"HORNN\"])\n",
        "    plt.show()\n",
        "    mse_test+=mean_squared_error(bb,cc)\n",
        "#    mm.append([X_plot_new_test[i],aa,bb,cc])\n",
        "print('total mse test:',mse_test/len(X_test))"
      ],
      "metadata": {
        "id": "whuFHOpQ9lDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}